---
output: html_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
# Load any necessary packages here

library(sf)
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(purrr)
library(parallel)
library(xgboost)
```

### Task 1 - Clean and Merge Data

Colin completed much of this task for us in class, including loading the parking data (and saving it so we wouldn't have to later reload it), and transforming the `pluto` data into the appropriate `sf` data type. What was left for us was to standardize the addresses in both dataframes as much as possible in order to maximize the rows we could get out of merging the two. This task was an exhibit in declining returns: some of the obvious stuff like getting rid of the ordinal text after street numbers (78 instead of 78th, for example) or turning words like "Avenue" to "AVE" helped increase our merge from 20,000 or so rows to well over a million, but after those adjustments edge-cases like converting "BWAY" to "BROADWAY" only yielded marginal improvements. Ultimately, we were able to find 1.6 million addresses that existed in both datasets.

#### Parking Violation Data

```{r, message = FALSE, warning = FALSE}
if (!file.exists("nyc_parking.Rdata")) {
  nyc_raw = read_csv("/data/nyc_parking/NYParkingViolations.csv") 
  
  nyc = nyc_raw %>%
    setNames(str_replace_all(names(.)," ", "_")) %>%
    dplyr::select(Violation_Precinct, House_Number, Street_Name) %>%
    transmute(precinct = Violation_Precinct, address = paste(House_Number, Street_Name)) %>%
    filter(address != "") %>%
    filter(precinct <= 34)
  
  save(nyc, file="nyc_parking.Rdata")
} else {
  load(file="nyc_parking.Rdata")
}
```

#### Geocoding Data

```{r, message = FALSE, warning = FALSE}
pluto = st_read("/data/nyc_parking/pluto_manhattan/MNMapPLUTO.shp", quiet=TRUE, stringsAsFactors = FALSE)

pluto = pluto %>%
  st_geometry() %>%
  st_centroid() %>%
  st_coordinates() %>% 
  data.frame(address = pluto$Address, ., stringsAsFactors = FALSE) %>%
  tbl_df() %>%
  filter(!is.na(address))
```


#### Clean and merge data

```{r, message = FALSE, warning = FALSE}
nyc$address = 
  toupper(nyc$address) %>%
  str_replace_all(.,"BLVD", "BL") %>%  ### turning road types into their abbreviations
  str_replace_all(.,"STREET|(STREE|STRT|STRE|STR)$","ST") %>%
  str_replace_all(., "AVENUE|(AVEN|AVENU|AV)$", "AVE") %>%
  str_replace_all(.,"EAST","E") %>% ### turning directions into their abbreviations
  str_replace_all(., "WEST", "W") %>%
  str_replace_all(.,"SOUTH","S") %>%
  str_replace_all(.,"NORTH","N") %>%
  str_replace_all(., "\\s(DRIVE|DRI|DRIV)$", " DR") %>%
  str_replace_all(.,"\\s(PLACE|PLAC|PLACES)$", " PL") %>%
  str_replace_all(.,"\\s(SQUARE|SQUAR|SQUARES|SQE)$", " SQ") %>%
  str_replace_all(., "AVE OF THE AMER", "6 AVE") %>% ### using common name for Avenue of Americas
  str_replace_all(., "(?<=\\d)(TH|ST|ND|RD)", "") %>% ### getting rid of ordinal text after digits
  str_replace_all(., "BWAY", "BROADWAY")

### creates boolean column for addresses likely missing the word "Ave" or "Street"
### and then pastes those words on
nyc =nyc %>%
  mutate(AVE = str_detect(address,"^[0-9]+\\s[0-9]+$")) %>%
  mutate(ST = str_detect(address,"^[0-9]+\\s(E|W|N|S)\\s?[0-9]+$")) %>%
  mutate(address = ifelse(AVE, paste(address,"AVE"), address))%>%
  mutate(address = ifelse(ST, paste(address,"ST"), address)) %>%
  dplyr::select(-c(AVE, ST))


pluto$address = 
  toupper(pluto$address) %>% 
  str_replace_all(.,"BLVD", "BL") %>% 
  str_replace_all(.,"STREET","ST") %>%
  str_replace_all(., "AVENUE", "AVE") %>%
  str_replace_all(.,"STREET|(STREE|STRT|STRE|STR)$","ST") %>%
  str_replace_all(.,"AVENUE|(AVEN|AVENU|AV)$", "AVE") %>%
  str_replace_all(.,"EAST","E") %>% 
  str_replace_all(., "WEST", "W") %>%
  str_replace_all(.,"SOUTH","S") %>%
  str_replace_all(.,"NORTH","N") %>%
  str_replace_all(., "\\s(DRIVE|DRI|DRIV|)$", " DR") %>%
  str_replace_all(.,"\\s(PLACE|PLAC|PLACES)$", " PL") %>%
  str_replace_all(.,"\\s(SQUARE|SQUAR|SQUARES|SQE)$", " SQ") %>%
  str_replace_all(., "AVE OF THE AMER", "6 AVE") %>%
  str_replace_all(., "(?<=\\d)(TH|ST|ND|RD)", "") %>%
  str_replace_all(., "BWAY", "BROADWAY")

d = inner_join(
  nyc,
  pluto,
  by="address"
)

manh_precincts = c(1,5,6,7,9,10,13,14,17,18,19,20,22,23,24,25,26,28,30,32,33,34)

d = filter(d, precinct %in% manh_precincts)

```


## Task 2 - Modeling

Again, Colin did a fair amount of this for us in class, including reading in the Manhattan shapefile and intersecting it over a grid of possible points. What was left for us to do in the first stage was account for Central Park, whose 22nd precinct is extremely underrepresented in the dataset since Central Park has no addresses within its borders and it's difficult to park your car in the middle of the Great Meadow. So for this section we were permitted to manually recreate the precinct's boundaries, which we did by finding the four corners of the rectangular park on Google Maps to create a closed polygon, and then binding its overlap with the manhattan points to the dataframe along with the 22nd precinct label. 

In terms of modeling, rumor had it that the best results could be obtained by focusing on gradient boosting with the `xgboost` package and tuning the parameters while also doubling back to work on particularly weak precincts based on the predictive results. We ran a bunch of different iterations with `xgboost` and determined that more extreme parameters (10 threads, 100 rounds) didn't fare much better on the in-sample data, but did lead to better scores when compared to the actual precinct boundaries, so we used those for our final model. 

### Setup

```{r, message = FALSE, warning = FALSE}

if (!file.exists("manh.Rdata")) {
    
  manh = st_read("/data/nyc_parking/nybb/nybb.shp", quiet=TRUE) %>%
    filter(BoroName == "Manhattan")
  
  bbox = st_bbox(manh)
  
  X = seq(bbox["xmin"], bbox["xmax"], 0.00075)
  Y = seq(bbox["ymin"], bbox["ymax"], 0.00075)
  
  grid = expand.grid(X=X, Y=Y) %>% 
    as.matrix() %>%
    st_multipoint() %>%
    st_sfc() %>%
    st_set_crs(st_crs(manh))
      
  manh_pts = st_intersection(st_geometry(manh), grid) %>% st_cast("POINT")
   
  manh_xy = st_coordinates(manh_pts) %>% as.data.frame()
  
  save(manh, manh_pts, manh_xy, file="manh.Rdata")
} else {
  load("manh.Rdata")
}

cp <- st_polygon(list(cbind(c(-73.958188, -73.949227, -73.972994, -73.981783,-73.958188),
                            c(40.800527, 40.796904, 40.764343, 40.768075, 40.800527)))) %>% 
  st_sfc() %>% st_set_crs(st_crs(manh))

cp_pts = data.frame(precinct = 22,
                    address = NA,
                    st_intersection(cp, manh_pts) %>% 
                      st_cast("POINT") %>% st_coordinates())

d = rbind(d, cp_pts)
```

### Gradient Boosting

```{r, message = FALSE, warning = FALSE}
d_xg = d %>% dplyr::select(-address) %>% mutate(precinct = as.factor(precinct))

precincts = d_xg$precinct %>% levels()
y = (d_xg$precinct %>% as.integer()) - 1L
x = d_xg %>% dplyr::select(X,Y) %>% as.matrix()

m = xgboost(data=x, label=y, nthread = 10, nround = 100, objective="multi:softmax", num_class=length(precincts),
            verbose = 0)
  
p_index = predict(m, newdata=as.matrix(manh_xy))
precinct_pred = precincts[p_index+1L] %>% as.character() %>% as.integer()
  
pred_df = cbind(manh_xy, precinct=precinct_pred)
  
pred_sf = st_sf(precinct = precinct_pred, geometry = manh_pts)

pred_sf_mp = pred_sf %>%
  dplyr::group_by(precinct) %>%
  dplyr::summarize(geometry = list(st_cast(geometry,"MULTIPOINT")))
  
pred_boundary = pred_sf_mp %>% st_buffer(0.00075) %>% st_buffer(-0.0005)

#devtools::install_github("tidyverse/ggplot2")

ggplot() +
  geom_sf(data=pred_boundary, aes(fill=as.factor(precinct)), alpha=0.3)

st_write(pred_boundary, "precincts.geojson", delete_dsn = TRUE, quiet=TRUE)
```
